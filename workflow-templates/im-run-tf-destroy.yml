# Workflow Code: HostileMacaw_v29    DO NOT REMOVE
# Purpose:
#    Destroys the resources created by a terraform configuration when someone kicks it off manually.
#
# Frequency:
#    - This workflow should only be used once per repository
#
# Projects to use this Template with:
#    -  Terraform (Optional Template)
#
# TODO Prerequisites
#    - Ensure each of the repo-level and env-level secrets used in this workflow have been populated by an admin in your repository.

name: Terraform Destroy
run-name: Terraform destroy ${{ inputs.root-module }} at ${{ inputs.branch-tag-sha }}
on:
  workflow_dispatch:
    inputs:
      # This is required because a tf init has to be performed before the terraform destroy command
      branch-tag-sha:
        description: The branch, tag or sha of the terraform that has the configuration of the resources that will be destroyed.
        required: true
      root-module:
        description: The directory containing the Terraform root module to destroy
        required: true
        type: choice
        options: # TODO: Update for the root modules that are available
          - dev
          - qa
          - stage
          - demo
          - uat
          - prod
          - stage-secondary
          - prod-secondary
      tf-targets:
        description: 'Targeted Destroys: format -target module.your-module.resource'
        required: false

permissions:
  # Required for secretless azure access and deploys
  id-token: write
  contents: read
  # Required for create-github-deployment (in the reusable update-github-deployments-and-send-teams-notification job)
  deployments: write
  actions: read

env:
  DEPLOYMENT_DESC: 'Destroying <project-name> ${{ inputs.tf-targets }} ${{ inputs.root-module }} via Terraform at ${{ inputs.branch-tag-sha }}' # TODO: Replace <project-name> with your project
  GITHUB_REF: ${{ inputs.branch-tag-sha  }}
  PLAN_STORAGE_CONTAINER: 'tfstate'
  # The following ARM_* values are env-level secrets/variables
  ARM_SUBSCRIPTION_ID: ${{ vars.ARM_SUBSCRIPTION_ID }}
  ARM_CLIENT_ID: ${{ vars.ARM_CLIENT_ID }}
  ARM_TENANT_ID: ${{ vars.ARM_TENANT_ID }} # This is an org-level variable
  ARM_ENVIRONMENT: 'public'
  TF_IN_AUTOMATION: 'true'
  TF_VERSION: '~>1.4.0' #TODO:  Verify your version of terraform.
  TF_WORKING_DIR: './infrastructure/${{ inputs.root-module }}' # TODO: Verify this directory is correct for your repository (older projects may not be inside of an infrastructure folder)
  # The following SSH_* secrets are org-level secrets
  SSH_KEY_STORAGE_ACCOUNT: ${{ secrets.SSH_STORAGE_ACCOUNT }}
  SSH_KEY_NETWORK_INFO: ${{ secrets.SSH_NETWORK_INFO }}
  SSH_KEY_AAD_GROUP_MEMBERS: ${{ secrets.SSH_AAD_GROUP_MEMBERS }}
  SSH_DEPLOY_KEY_INFO: |
    [
      { "orgAndRepo": "im-platform/storage-account-network-rules", "envName" : "SSH_KEY_STORAGE_ACCOUNT" },
      { "orgAndRepo": "im-platform/network-information", "envName" : "SSH_KEY_NETWORK_INFO" },
      { "orgAndRepo": "im-platform/aad-group-members", "envName" : "SSH_KEY_AAD_GROUP_MEMBERS" }
    ]

jobs:
  # This job utilizes a reusable workflow which will:
  #   1 - Verify the branch/tag/sha provided is a valid ref.
  #   2 - If deploying to a production environment, verify the tag is reachable from the default branch.
  setup-deployment-workflow:
    uses: im-practices/.github/.github/workflows/im-reusable-setup-deployment-workflow.yml@v3
    with:
      ref-to-deploy: ${{ inputs.branch-tag-sha }}
      deployment-environment: ${{ inputs.root-module }}
      verify-release-production-ready: false # Set to false since terraform projects generally do not have an associated release
      # production-environments: 'prod,prod-secondary'  # TODO:  Adjust and include the production-environments if necessary (some apps may need to add stage/stage-secondary to this list)
      # default-branch: main # TODO:  Update and include this arg if the default branch is not main
      # workflow-summary : | # TODO:  If desired, the workflow summary that is generated can be overridden by providing this custom value.

  set-vars:
    needs: [setup-deployment-workflow]
    runs-on: ubuntu-latest # Force this to run on github-hosted runner by using a tag that does not exist on self-hosted runners

    outputs:
      # To use these values: ${{ needs.set-vars.outputs.<OUTPUT_NAME> }}
      TARGET_RESOURCE_GROUP: ${{ steps.set-variables.outputs.TARGET_RESOURCE_GROUP }}
      PRIMARY_RESOURCE_GROUP: ${{ steps.set-variables.outputs.PRIMARY_RESOURCE_GROUP }}
      STORAGE_ACCOUNT: ${{ steps.set-variables.outputs.STORAGE_ACCOUNT }}
      GITHUB_SECRETS_ENVIRONMENT: ${{ steps.set-variables.outputs.GITHUB_SECRETS_ENVIRONMENT }}

    steps:
      # For more information and best practices on the usage and options available
      # for this action go to: https://github.com/im-open/set-environment-variables-by-scope#usage-instructions
      - name: Set Variables
        id: set-variables
        uses: im-open/set-environment-variables-by-scope@v1.1
        with:
          scope: ${{ inputs.root-module }}
          create-output-variables: true
        env:
          # Resource group you are targeting for deploy.  Also this variable is used to delete and re-create azure locks.
          # TODO: Add the NA27 (West Central US) Resource Group to the stage-secondary/prod-secondary to the variables.
          # TODO: Add the NA26 (West US2) Resource Groups to dev/qa/stage/demo/uat/prod to the variables
          TARGET_RESOURCE_GROUP@dev: ''
          TARGET_RESOURCE_GROUP@qa: ''
          TARGET_RESOURCE_GROUP@stage: ''
          TARGET_RESOURCE_GROUP@stage-secondary: ''
          TARGET_RESOURCE_GROUP@demo: ''
          TARGET_RESOURCE_GROUP@uat: ''
          TARGET_RESOURCE_GROUP@prod: ''
          TARGET_RESOURCE_GROUP@prod-secondary: ''

          # Resource group holding the state storage account and managed service identities
          # TODO: Add the Stage/Prod NA26 (West US2) Resource Groups below.
          PRIMARY_RESOURCE_GROUP@dev: ''
          PRIMARY_RESOURCE_GROUP@qa: ''
          PRIMARY_RESOURCE_GROUP@stage stage-secondary: ''
          PRIMARY_RESOURCE_GROUP@demo: ''
          PRIMARY_RESOURCE_GROUP@prod prod-secondary: ''

          # This variable is used to upload and download blobs from blob storage
          # TODO: Add the NA26 (West US2) Storage Accounts to the variables
          STORAGE_ACCOUNT@dev: ''
          STORAGE_ACCOUNT@qa: ''
          STORAGE_ACCOUNT@stage stage-secondary: ''
          STORAGE_ACCOUNT@demo: ''
          STORAGE_ACCOUNT@uat: ''
          STORAGE_ACCOUNT@prod prod-secondary: ''

          # Used for getting Azure Credentials Secrets
          GITHUB_SECRETS_ENVIRONMENT@dev qa stage prod demo: '${{ inputs.root-module }}'
          GITHUB_SECRETS_ENVIRONMENT@stage-secondary: 'stage'
          GITHUB_SECRETS_ENVIRONMENT@prod-secondary: 'prod'

  # Each env has their own stakeholder approval environment.  If no required reviewers are set for
  # that environment, the workflow will continue without requiring anyone to approve the deployment.
  stakeholder-approval:
    needs: [set-vars]
    runs-on: ubuntu-latest # Force this to run on github-hosted runner by using a tag that does not exist on self-hosted runners
    environment: '${{ needs.set-vars.outputs.GITHUB_SECRETS_ENVIRONMENT }} Stakeholder Approval' # Use inputs context because env context is not available to environment:
    steps:
      - name: Approval Received
        run: echo "Stakeholder approval was received"

  tf-plan:
    needs: [set-vars, stakeholder-approval]
    runs-on: im-linux
    environment: ${{ needs.set-vars.outputs.GITHUB_SECRETS_ENVIRONMENT }}

    defaults:
      run:
        shell: bash
        working-directory: '${{ env.TF_WORKING_DIR }}'

    outputs:
      tf_plan_name: ${{ steps.upload.outputs.tf_plan_name }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          ref: ${{ env.GITHUB_REF }}

      # Allows pulling modules from the repo instead of artifactory
      - name: Setup SSH Keys and known_hosts
        uses: im-open/setup-deploy-keys@v1.1
        with:
          deploy-key-info: ${{ env.SSH_DEPLOY_KEY_INFO }}

      - name: AZ Login
        uses: azure/login@v2
        with:
          # This is an org-level variable
          tenant-id: ${{ vars.ARM_TENANT_ID }}
          # These are env-level variables
          subscription-id: ${{ vars.ARM_SUBSCRIPTION_ID }}
          client-id: ${{ vars.ARM_CLIENT_ID }}

      - name: Setup Terraform
        id: setup
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: '${{ env.TF_VERSION }}'

      - name: Terraform Init
        id: init
        run: terraform init

      # TODO: Remove the pagerduty token if not configuring pagerduty.  If using pagerduty verify 'pagerduty_token' is the name of the variable that tf accepts as the variable
      # TODO: Add any other secrets that would be required for a tf plan to succeed.  Since this is a multi-line command every line except the last will need a \ on the end of it
      # PAGERDUTY_API_KEY is an org-level secret
      # This will run a plan and output it to a file.  The file is then uploaded to azure storage so it can be used later in the apply.
      - name: Terraform Plan
        id: plan
        run: |
          mkdir plans
          terraform plan -destroy -no-color -lock-timeout=90s \
            -var="pagerduty_token=${{ secrets.PAGERDUTY_API_KEY }}" \
            -out=./plans/tfplan ${{ inputs.tf-targets }}

      - name: Output Plan Results Summary
        id: plan-results-output
        continue-on-error: true
        run: |
          plan=$(cat <<'EOF'
          ${{ format('{0}{1}', steps.plan.outputs.stdout, steps.plan.outputs.stderr) }}
          EOF
          )

          plan_results=$(echo "$plan" | grep -e "Plan:" -e "No changes." -e "# module.") # TODO: Add any unique filters required in your pipeline.  IE terraform-f5 output doesn't have module. prefix.
          echo "Plan Results Returned:"
          echo "$plan_results "
          if [[ -z "$plan_results" ]]; then
            plan_results="Errors were found in terraform plan"
          fi

          cat >> $GITHUB_STEP_SUMMARY << EOL
          ## Plan Results Summary [${{ inputs.root-module }}]

          \`\`\`js
          $plan_results
          \`\`\`
          EOL

      - name: Upload plan to blob storage
        id: upload
        shell: pwsh
        run: |
          $terraformPlanName = "$(Get-Date -Format 'yyyyMMdd-HHmmss').plan.zip"
          $terraformBlobName = "plans/$terraformPlanName"

          Add-Type -Assembly "System.IO.Compression.FileSystem"
          [System.IO.Compression.ZipFile]::CreateFromDirectory("plans", $terraformPlanName)

          echo "Terraform Plan Name: $terraformPlanName"
          echo "current directory:"
          ls -R

          echo "Uploading tf plan to azure storage account ${{ needs.set-vars.outputs.STORAGE_ACCOUNT }}"
          az storage blob upload --no-progress --auth-mode login --account-name ${{ needs.set-vars.outputs.STORAGE_ACCOUNT }} --container-name ${{ env.PLAN_STORAGE_CONTAINER }} --file $terraformPlanName --name $terraformBlobName
          echo "The plan was successfully uploaded"

          "tf_plan_name=$terraformPlanName" | Out-File -FilePath $env:GITHUB_OUTPUT -Append

      - name: Azure logout
        run: |
          az logout
          az cache purge
          az account clear

  # This job targets the Terraform Approval environment.  This will break the workflow and give one of the
  # required reviewers for this environment a chance to look at the plan in the previous job and approve it.
  tf-plan-manual-approval:
    needs: [set-vars, tf-plan]
    runs-on: ubuntu-latest # Force this to run on github-hosted runner by using a tag that does not exist on self-hosted runners
    environment: 'Terraform Approval' # TODO: Add required reviewers to this environment in GitHub.  This should be anyone who can review a terraform plan and proceed with the deployment
    steps:
      - name: Approval Received
        run: echo "Approval on the tf plan was received"

  tf-apply:
    needs: [set-vars, tf-plan, tf-plan-manual-approval, stakeholder-approval]
    runs-on: im-linux
    environment: ${{ needs.set-vars.outputs.GITHUB_SECRETS_ENVIRONMENT }}

    defaults:
      run:
        shell: bash
        working-directory: '${{ env.TF_WORKING_DIR }}'

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          ref: ${{ env.GITHUB_REF }}

      # Allows pulling modules from the repo instead of artifactory
      - name: Setup SSH Keys and known_hosts
        uses: im-open/setup-deploy-keys@v1.1
        with:
          deploy-key-info: ${{ env.SSH_DEPLOY_KEY_INFO }}

      - name: AZ Login
        uses: azure/login@v2
        with:
          # This is an org-level variable
          tenant-id: ${{ vars.ARM_TENANT_ID }}
          # These are env-level variables
          subscription-id: ${{ vars.ARM_SUBSCRIPTION_ID }}
          client-id: ${{ vars.ARM_CLIENT_ID }}

      - name: Setup Terraform
        id: setup
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: '${{ env.TF_VERSION }}'

      - name: Download blob
        shell: pwsh
        run: |
          mkdir plans
          echo "Current working directory: $pwd"
          $terraformBlobName = "plans/${{ needs.tf-plan.outputs.tf_plan_name }}"
          echo "The blob name is: $terraformBlobName"

          Write-Host "Download blob to ./plans"
          az storage blob download --no-progress --auth-mode login --account-name ${{ needs.set-vars.outputs.STORAGE_ACCOUNT }} --container-name ${{ env.PLAN_STORAGE_CONTAINER }} --file $pwd/$terraformBlobName --name $terraformBlobName

          try {
            [System.IO.Compression.ZipFile]::ExtractToDirectory("$pwd/$terraformBlobName", "$pwd/plans")
          }
          catch {
            # Even though it hits this catch block the archive is extracted as expected.  No good explanation.
          }

          Write-Host "Zip extracted"

      # TODO: Uncomment if you have azure locks in stage and prod
      # - name: Delete RGRP Azure Locks
      #   id: remove-locks
      #   if: needs.set-vars.outputs.GITHUB_SECRETS_ENVIRONMENT == 'prod' || needs.set-vars.outputs.GITHUB_SECRETS_ENVIRONMENT == 'stage'
      #   shell: pwsh
      #   env:
      #     LOCK_NAME: ${{ needs.set-vars.outputs.TARGET_RESOURCE_GROUP }}-delete-locks
      #     RGRP: ${{ needs.set-vars.outputs.TARGET_RESOURCE_GROUP }}
      #   run: |
      #     az group lock delete --name $Env:LOCK_NAME --resource-group $Env:RGRP
      #     While(
      #       $(az group lock list --resource-group $Env:RGRP --output tsv --query "[?name=='${$Env:LOCK_NAME}'].id")
      #     ){
      #       Start-Sleep -s 0.5
      #     }

      - name: Terraform Init
        id: init
        run: terraform init

      - name: Terraform Destroy
        run: terraform apply -auto-approve -no-color -lock-timeout=90s -input=false ./plans/tfplan

      # TODO: Uncomment if you have azure locks in stage and prod
      # - name: Add RGRP Azure Locks
      #   id: add-locks
      #   if: needs.set-vars.outputs.GITHUB_SECRETS_ENVIRONMENT == 'prod' || needs.set-vars.outputs.GITHUB_SECRETS_ENVIRONMENT == 'stage'
      #   run: |
      #     az group lock create --name "${{ needs.set-vars.outputs.TARGET_RESOURCE_GROUP }}-delete-locks"  --resource-group "${{ needs.set-vars.outputs.TARGET_RESOURCE_GROUP }}" --lock-type CanNotDelete

      - name: Azure logout
        run: |
          az logout
          az cache purge
          az account clear

  # This job utilizes a reusable workflow which will:
  #  1 - Update the deployment board based on the workflow conclusion
  #  2 - Post a deployment status in the repo owner's Teams channel (connected to the MS_TEAMS_URI secret)
  #  3 - Post a deployment status in the Deployment Notifications Teams channel if the deploy is for prod, is successful and the flag to do so is not set to false
  update-github-deployments-and-send-teams-notification:
    needs: [set-vars, tf-apply]
    if: always()
    uses: im-practices/.github/.github/workflows/im-reusable-finish-deployment-workflow.yml@v3
    with:
      # Required Inputs
      deployment-environment: ${{ inputs.root-module }} # The environment/target that was deployed to (dev, qa, stage, stage-secondary, uat, demo, prod, prod-secondary)
      gh-secrets-environment: ${{ needs.set-vars.outputs.GITHUB_SECRETS_ENVIRONMENT }} # The GitHub environment that secrets are pulled from
      release-tag: ${{ inputs.branch-tag-sha }}
      title-of-teams-post: 'Destroy <<PROJECT_NAME>> ${{ inputs.tf-targets }} @${{ inputs.branch-tag-sha }} in ${{ inputs.root-module }}' # TODO:  Replace <<PROJECT_NAME>> and verify title to ensure it is descriptive/readable.
      post-status-in-deployment-notifications-channel: false
      ms-teams-uri: ${{ vars.MS_TEAMS_URI }}
      deploy-notifications-channel: ${{ vars.DEPLOY_NOTIFICATIONS_CHANNEL }}
      entity: '' # TODO: The TechHub (catalog-info.yml) entity that is being deployed.  This value should match the metadata.name of the entity defined in catalog-info.yml.
      instance: '' # TODO: A specific target deployment location e.g., primary, primary-app-service, testing-slot, failover-slot-2, NA26, NA27-production-slot.  Generally some combination of deployment-environment, slot-name and AZ region values.

      # Optional Inputs with their default values.  These items can be removed if the default value does not need to be adjusted.
      # timezone: 'america/denver'                            # TODO:  Include this argument and update if your timezone is not america/denver
      # TODO:  These are the custom facts that will be included the different Teams posts by default.  If adjusting the facts that are supplied, they must be a valid JSON array.
      # custom-facts-for-team-channel: |
      #   [
      #     { "name": "Workflow", "value": "${{ github.workflow }}" },
      #     { "name": "Run", "value": "${{ github.run_id }}" },
      #     { "name": "Actor", "value": "${{ github.actor }}" },
      #     { "name": "Version", "value": "${{ inputs.branch-tag-sha }}" }
      #   ]
      # custom-facts-for-deployment-notifications-channel: |
      #   [
      #     { "name": "Actor", "value": "${{ github.actor }}" },
      #     { "name": "Version", "value": "${{ inputs.branch-tag-sha }}" }
      #   ]
